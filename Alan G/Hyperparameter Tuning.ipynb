{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ba021-f472-4c9b-a3a3-94898c32f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/venv/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/venv/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/venv/lib/python3.10/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "user_train_df = pd.read_csv(\"../MATHM0029_2024_TB-1/user_train_df.csv\")\n",
    "user_test_df = pd.read_csv(\"../MATHM0029_2024_TB-1/user_test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e2422-b447-41d1-89f5-0694b0853d93",
   "metadata": {},
   "source": [
    "# Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7479661d-b7b0-4559-801e-2ba58aeac4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/25 09:42:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/25 09:42:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/25 09:42:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, collect_list, explode\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RecommenderSystem\").getOrCreate()\n",
    "\n",
    "# Prepare the data\n",
    "columns = [\"User ID\", \"Item ID\", \"Rating\"]\n",
    "user_train_spark_df = spark.createDataFrame(user_train_df[columns])\n",
    "\n",
    "# ALS Model Setup\n",
    "als = ALS(\n",
    "    userCol=\"User ID\",\n",
    "    itemCol=\"Item ID\",\n",
    "    ratingCol=\"Rating\",\n",
    "    maxIter=10, # Number of iterations - Ensures the algorithm has enough time to converge.\n",
    "    regParam=0.1, # Regularization parameter - Tests different levels of regularization to prevent overfitting.\n",
    "    rank=10, # Number of latent factors - This explores different complexities of the model.\n",
    "    coldStartStrategy=\"drop\"  # Prevent NaN predictions\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "als_model = als.fit(user_train_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f7e071-f259-4983-b938-988952a2c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6934949132375731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with RMSE on test set\n",
    "user_test_spark_df = spark.createDataFrame(user_test_df[columns])\n",
    "predictions = als_model.transform(user_test_spark_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"Rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d2990-2954-4a46-a576-b59fac5df4a4",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ea03f-5ad9-4231-a342-983f60da5a23",
   "metadata": {},
   "source": [
    "Hereâ€™s what each parameter in the ALS model means:\n",
    "\n",
    "1. maxIter\n",
    "Description: This sets the maximum number of iterations for the ALS optimization process.\n",
    "Purpose: ALS is an iterative algorithm, and this parameter controls how many iterations the algorithm will perform to converge to a solution.\n",
    "Impact:\n",
    "Increasing maxIter allows the algorithm to potentially achieve a better approximation of the latent factors, improving recommendations.\n",
    "However, too many iterations might lead to overfitting or unnecessary computational expense if convergence happens early.\n",
    "\n",
    "2. regParam\n",
    "Description: This specifies the regularization parameter used to prevent overfitting.\n",
    "Purpose: Regularization helps control the magnitude of the learned latent factors to avoid overfitting to the training data.\n",
    "Impact:\n",
    "Lower regParam values may lead to better fitting of the training data but risk overfitting and poor generalization to test data.\n",
    "Higher regParam values increase the regularization effect, which may result in underfitting if set too high.\n",
    "\n",
    "3. rank\n",
    "Description: This defines the number of latent factors (or features) in the ALS model.\n",
    "Purpose: The latent factors represent the dimensions in which user and item preferences are mapped. A higher rank allows for a more complex model that can capture nuanced relationships.\n",
    "Impact:\n",
    "A larger rank increases the model's capacity to represent complex patterns but also increases computational cost and the risk of overfitting.\n",
    "A smaller rank might lead to underfitting if it cannot capture sufficient detail in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a51604d-77da-4923-8921-4934024152e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters:\n",
      " - Rank: 10\n",
      " - MaxIter: 10\n",
      " - RegParam: 0.1\n",
      "RMSE on Test Data: 0.6934949132375731\n"
     ]
    }
   ],
   "source": [
    "# Note takes a while to run\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a parameter grid for tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [5, 10, 15]) \\\n",
    "    .addGrid(als.maxIter, [10, 20]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3  # Use 3-fold cross-validation\n",
    ")\n",
    "\n",
    "# Fit cross-validation to the training data\n",
    "cv_model = cv.fit(user_train_spark_df)\n",
    "\n",
    "# Find the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "predictions = best_model.transform(user_test_spark_df)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Best Model Parameters:\\n - Rank: {best_model.rank}\\n - MaxIter: {best_model._java_obj.parent().getMaxIter()}\\n - RegParam: {best_model._java_obj.parent().getRegParam()}\")\n",
    "print(f\"RMSE on Test Data: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248c7f5-ef60-46f8-8af7-6a269f823e14",
   "metadata": {},
   "source": [
    "Parameter Grid systematically explores a range of values for each hyperparameter to tune the ALS model.\n",
    "\n",
    "Cross-validation helps avoid overfitting to a specific train/test split and ensures the selected parameters generalize well.\n",
    "\n",
    "After tuning, we've selected the best model based on the lowest RMSE during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb5eb1-ceca-4eb4-820e-03c1b2c6a1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
