{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6524aa57-56ef-4f00-a2cf-6b4a79490666",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix Factorization is a powerful technique for building recommendation systems, especially when working with sparse user-item interaction data. One popular approach to matrix factorization is Alternating Least Squares (ALS), which efficiently handles large-scale datasets by decomposing a user-item interaction matrix into two low-rank matrices: one representing user preferences and the other item attributes.\n",
    "\n",
    "We decided to explore both ALS and SVD for our project.\n",
    "For our goals, we decided that ALS is generally the better choice because:\n",
    "\n",
    "1) It is optimized for sparse, explicit feedback datasets.\n",
    "2) It scales well with larger datasets and distributed computation.\n",
    "3) It includes regularization and implicit feedback handling, making it versatile for different recommendation scenarios.\n",
    "\n",
    "However, if you have a specific need to explore latent factors in-depth, or you're working with dense data and non-distributed settings, SVD could also be a viable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8519a4c-9935-4bbf-b75e-f54e4922b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/venv/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/venv/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/venv/lib/python3.10/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "user_train_df = pd.read_csv('../Katherine W/dataSets/user_train_df.csv')\n",
    "user_test_df = pd.read_csv('../Katherine W/dataSets/user_test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3e43b-f045-46df-8e6a-74ed8dbc0ea1",
   "metadata": {},
   "source": [
    "We use a user-based 80-20 train-test split, where 80% of each user's ratings (earliest timestamps) are included in the training set, and the remaining 20% (latest timestamps) are in the test set. \n",
    "We also explored a global 80-20 split, where the first 80% of all ratings is included in the training set, and the remaining 20% are in the test set.\n",
    "\n",
    "We decided to use a user-based 80-20 split for train-test separation as it\n",
    "1) Ensures that each user has both training and test data, enabling the model to learn and test for every individual.\n",
    "2) The impact of highly active users (with many ratings) is contained, preventing them from dominating the training process and skewing the test results.\n",
    "\n",
    "However, a major drawback of user-based split is that In systems with large user bases, a user split may require additional computation to manage individual user datasets, especially for training models like matrix factorization.\n",
    "\n",
    "## ALS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d1ccc-e00f-41f8-a20a-57f9a85d132e",
   "metadata": {},
   "source": [
    "The decision to drop variables such as user information (e.g., age, occupation, gender) and item information (e.g., genre) in the following model can be explained by the following considerations:\n",
    "1) The ALS (Alternating Least Squares) algorithm used in the model is inherently a collaborative filtering approach. Collaborative filtering relies only on user-item interaction data (e.g., user IDs, item IDs, and ratings) to make predictions. The assumption is that similar users (based on their historical preferences) and similar items (based on their popularity among users) can be identified purely from the interaction matrix. Including demographic or item-specific data is not essential in this paradigm.\n",
    "2) ALS models are designed to handle sparse user-item interaction matrices efficiently. Adding additional features like user demographics or item metadata would require incorporating these into the matrix factorization process, significantly increasing computational complexity.\n",
    "3) The ALS algorithm, in its standard implementation, does not natively support incorporating additional side information like user or item metadata. While hybrid approaches exist (e.g., feature-enhanced matrix factorization or deep learning models), they require additional preprocessing, modeling, and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f89e86-3035-480c-90bf-66e00e17679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/25 13:15:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, collect_list, explode\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RecommenderSystem\").getOrCreate()\n",
    "\n",
    "# Prepare the data\n",
    "columns = [\"User ID\", \"Item ID\", \"Rating\"]\n",
    "user_train_spark_df = spark.createDataFrame(user_train_df[columns])\n",
    "\n",
    "# ALS Model Setup\n",
    "als = ALS(\n",
    "    userCol=\"User ID\",\n",
    "    itemCol=\"Item ID\",\n",
    "    ratingCol=\"Rating\",\n",
    "    maxIter=10, # Number of iterations - Ensures the algorithm has enough time to converge.\n",
    "    regParam=0.1, # Regularization parameter - Tests different levels of regularization to prevent overfitting.\n",
    "    rank=10, # Number of latent factors - This explores different complexities of the model.\n",
    "    coldStartStrategy=\"drop\"  # Prevent NaN predictions\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "als_model = als.fit(user_train_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c2d34-3d7e-499a-ba0d-0b17ddf1f94b",
   "metadata": {},
   "source": [
    "We have now trained our ALS model, note that optimised parameter values are used in the model. The method is shown in \"Hyperparameter Tuning\" in the Alan G folder.\n",
    "\n",
    "Now we will generate a few recommendations to check that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3faa35-45e9-4bba-b742-50c2ee9039e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------+---------+\n",
      "|User ID|Movie Title                           |Rating   |\n",
      "+-------+--------------------------------------+---------+\n",
      "|1      |Pather Panchali (1955)                |5.1304927|\n",
      "|1      |Boys, Les (1997)                      |5.099012 |\n",
      "|1      |Angel Baby (1995)                     |4.9815707|\n",
      "|1      |Faust (1994)                          |4.964393 |\n",
      "|1      |Anna (1996)                           |4.8791194|\n",
      "|2      |Mina Tannenbaum (1994)                |5.5116477|\n",
      "|2      |Angel Baby (1995)                     |5.216749 |\n",
      "|2      |L.A. Confidential (1997)              |5.071832 |\n",
      "|2      |Whole Wide World, The (1996)          |4.9777217|\n",
      "|2      |Spanish Prisoner, The (1997)          |4.8399215|\n",
      "|3      |Santa with Muscles (1996)             |3.6494858|\n",
      "|3      |Return of the Jedi (1983)             |3.5813563|\n",
      "|3      |Boys, Les (1997)                      |3.5257242|\n",
      "|3      |Anne Frank Remembered (1995)          |3.4299893|\n",
      "|3      |Princess Bride, The (1987)            |3.4045403|\n",
      "|4      |Bitter Sugar (Azucar Amargo) (1996)   |6.3198733|\n",
      "|4      |American Dream (1990)                 |5.791051 |\n",
      "|4      |Whole Wide World, The (1996)          |5.7498097|\n",
      "|4      |Some Mothers Son (1996)               |5.4534388|\n",
      "|4      |Delta of Venus (1994)                 |5.4253864|\n",
      "|5      |Faust (1994)                          |4.8239307|\n",
      "|5      |Wrong Trousers, The (1993)            |4.583471 |\n",
      "|5      |Santa with Muscles (1996)             |4.425718 |\n",
      "|5      |Monty Python and the Holy Grail (1974)|4.4018664|\n",
      "|5      |Duck Soup (1933)                      |4.401797 |\n",
      "+-------+--------------------------------------+---------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the Item ID to Movie Title mapping\n",
    "item_to_title_mapping = user_train_df[[\"Item ID\", \"Movie Title\"]].drop_duplicates()\n",
    "\n",
    "# Convert the mapping to a Spark DataFrame for integration with recommendations\n",
    "item_to_title_spark_df = spark.createDataFrame(item_to_title_mapping)\n",
    "\n",
    "# Generate recommendations for all users\n",
    "user_recommendations = als_model.recommendForAllUsers(5)  # Top 5 recommendations per user\n",
    "\n",
    "# Explode the recommendations and map Item IDs to Movie Titles\n",
    "exploded_recommendations = user_recommendations.withColumn(\"recommendation\", explode(col(\"recommendations\")))\n",
    "exploded_recommendations = exploded_recommendations.withColumn(\"Item ID\", col(\"recommendation\").getItem(\"Item ID\")) \\\n",
    "                                                   .withColumn(\"Rating\", col(\"recommendation\").getItem(\"Rating\")) \\\n",
    "                                                   .drop(\"recommendation\")\n",
    "\n",
    "# Join with the movie titles\n",
    "recommendations_with_titles = exploded_recommendations.join(item_to_title_spark_df, on=\"Item ID\", how=\"inner\")\n",
    "\n",
    "from pyspark.sql.functions import split, trim\n",
    "\n",
    "# Split the Movie Title column to extract only the relevant part before the '|'\n",
    "recommendations_cleaned = recommendations_with_titles.withColumn(\n",
    "    \"Movie Title\", \n",
    "    trim(split(col(\"Movie Title\"), r\"\\|\").getItem(0))  # Get the first part before the '|'\n",
    ")\n",
    "\n",
    "# Display cleaned recommendations for 5 users\n",
    "recommendations_cleaned.select(\"User ID\", \"Movie Title\", \"Rating\") \\\n",
    "                       .orderBy(col(\"User ID\").asc(), col(\"Rating\").desc()) \\\n",
    "                       .show(25, truncate=False)  # Show 5 users, 5 movies each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c7d28-4e3c-4163-bdac-f7bef0e4d0b7",
   "metadata": {},
   "source": [
    "The rating values represent predicted preferences for movies, estimated by the ALS model. They are used to rank recommendations, with higher scores indicating a higher predicted preference for a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599a8f12-0d62-43d1-a270-4f8c05383e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.48037015142478745\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with MSE on test set\n",
    "user_test_spark_df = spark.createDataFrame(user_test_df[columns])\n",
    "predictions = als_model.transform(user_test_spark_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"mse\",\n",
    "    labelCol=\"Rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09bb39a-cca6-4741-ac2b-0287f5a37af7",
   "metadata": {},
   "source": [
    "We decided to use MSE (Mean Squared Error) as the performance metric as\n",
    "1) MSE is a standard metric in regression and recommendation tasks, making it easier to compare models across studies and datasets.\n",
    "2) It is sensitive to the variance in data, which can highlight models that consistently make large errors on certain predictions.\n",
    "\n",
    "We have also generated the MSE value for a global 80-20 train-test split in \"Global-ALS\" in the Alan G folder. We have obtained a lower MSE value for the user-based split which suggests that the user-based split might align better with individual preferences, leading to a lower MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a24fb5-5549-465c-8086-8e6d24e214d0",
   "metadata": {},
   "source": [
    "## Larger Scale Datasets\n",
    "\n",
    "The ALS model in Spark can handle large-scale datasets, but as the dataset size increases by a factor of 1000, computational challenges (e.g., memory requirements, training time) and data sparsity issues become more pronounced. For example, in Spark, ALS requires shuffling during each iteration to update the latent factors. As the dataset grows, this process becomes more time-consuming and memory-intensive, which could create bottlenecks in the training process. The network traffic required for shuffling data between nodes increases, which can negatively impact training time.\n",
    "\n",
    "To address these issues, optimizations like increasing cluster resources, tuning hyperparameters efficiently, and improving data partitioning will be essential. This means increasing the number of executors, executor memory, and number of cores per executor in the Spark cluster configuration.\n",
    "\n",
    "Other matrix factorization techniques, such as SGD, GPU-accelerated ALS, and also applying deep learning methods, can also be more suitable for extremely large datasets, depending on the specific requirements and infrastructure available. Each method comes with its trade-offs in terms of computational complexity, memory efficiency, and training time. For example, GPU-accelerated ALS is an extension of the standard ALS algorithm where matrix factorization is performed on GPUs instead of CPUs. This can significantly speed up the process of training ALS models, especially for large datasets, by taking advantage of parallel computation. However, GPU-based solutions require specialized hardware and may involve additional complexity in setting up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fc855-30dd-4d8e-b41e-32b5acdad87f",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1) yuefeng.zhang@pku.edu.cn, Y.Z. (no date) An introduction to matrix factorization and factorization machines in recommendation system, and beyond, ar5iv. Available at: https://ar5iv.org/html/2203.11026 (Accessed: 25 November 2024). \n",
    "\n",
    "2) 21.3. matrix factorization¶ colab [pytorch] open the notebook in colab colab [mxnet] open the notebook in colab colab [jax] open the notebook in colab colab [tensorflow] open the notebook in colab sagemaker studio lab open the notebook in SageMaker Studio Lab (no date b) 21.3. Matrix Factorization - Dive into Deep Learning 1.0.3 documentation. Available at: https://www.d2l.ai/chapter_recommender-systems/mf.html (Accessed: 25 November 2024). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c381-a4e6-4263-a269-d267d9d715ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
